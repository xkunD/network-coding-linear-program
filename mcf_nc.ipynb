{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution status: optimal\n",
      "Maximum concurrent flow: 0.7500\n",
      "\n",
      "Session 0: 0 → 1 (flow: 0.7500)\n",
      "  Path: 0 → 3 → 1, Flow: 0.3750\n",
      "  Path: 0 → 4 → 1, Flow: 0.3750\n",
      "  Total session flow: 0.7500\n",
      "\n",
      "Session 1: 1 → 2 (flow: 0.7500)\n",
      "  Path: 1 → 3 → 2, Flow: 0.3750\n",
      "  Path: 1 → 4 → 2, Flow: 0.3750\n",
      "  Total session flow: 0.7500\n",
      "\n",
      "Session 2: 0 → 2 (flow: 0.7500)\n",
      "  Path: 0 → 3 → 2, Flow: 0.3750\n",
      "  Path: 0 → 4 → 2, Flow: 0.3750\n",
      "  Total session flow: 0.7500\n",
      "\n",
      "Session 3: 3 → 4 (flow: 0.7500)\n",
      "  Path: 3 → 0 → 4, Flow: 0.2500\n",
      "  Path: 3 → 1 → 4, Flow: 0.2500\n",
      "  Path: 3 → 2 → 4, Flow: 0.2500\n",
      "  Total session flow: 0.7500\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "class MaxConcurrentFlowSolver:\n",
    "    \"\"\"\n",
    "    Finds the maximum flow that all sessions can achieve simultaneously.\n",
    "\n",
    "    Solving Fractional Multi Commodity Flow is Polynomial Time.\n",
    "    However, the print_solution method itself is NP cuz simple path finding.\n",
    "    \n",
    "    Solving Integral Multi Commodity Flow is NP hard.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, G: nx.Graph, sessions: List[Tuple[int, int]]):\n",
    "        \"\"\"\n",
    "        Initialize the solver with a network and sessions.\n",
    "        \n",
    "        Args:\n",
    "            G: NetworkX undirected graph with edge capacities stored as 'capacity' attribute\n",
    "            sessions: List of (source, target) tuples representing commodity flows\n",
    "        \"\"\"\n",
    "        self.G = G.copy()\n",
    "        self.sessions = sessions\n",
    "        self.num_nodes = G.number_of_nodes()\n",
    "        self.num_edges = G.number_of_edges()\n",
    "        self.num_commodities = len(sessions)\n",
    "        \n",
    "        # Store edges in a consistent order\n",
    "        self.edges = list(G.edges())\n",
    "        self.edge_index = {e: i for i, e in enumerate(self.edges)}\n",
    "        \n",
    "        # Add reverse edges to the edge index for undirected graph\n",
    "        for (u, v) in list(self.edge_index.keys()):\n",
    "            self.edge_index[(v, u)] = self.edge_index[(u, v)]\n",
    "        \n",
    "        # Extract edge capacities\n",
    "        self.capacities = np.array([G[u][v].get('capacity', float('inf')) for u, v in self.edges])\n",
    "        \n",
    "        # Create a directed graph for tracking flows (to handle edge directions properly)\n",
    "        self.flow_graph = nx.DiGraph()\n",
    "        for u, v in self.edges:\n",
    "            capacity = G[u][v].get('capacity', float('inf'))\n",
    "            self.flow_graph.add_edge(u, v, capacity=capacity)\n",
    "            self.flow_graph.add_edge(v, u, capacity=capacity)\n",
    "        \n",
    "        # Initialize results\n",
    "        self.flows = None\n",
    "        self.max_concurrent_flow = None\n",
    "        self.status = None\n",
    "    \n",
    "    def solve(self):\n",
    "        \"\"\"\n",
    "        Solve the maximum concurrent flow problem using linear programming.\n",
    "        \n",
    "        Returns:\n",
    "            Dict containing solution status and optimal flows\n",
    "        \"\"\"\n",
    "        # Create flow variables for each commodity on each edge\n",
    "        # f[k][i][j] represents flow of commodity k on edge (i,j)\n",
    "        f = {}\n",
    "        for k in range(self.num_commodities):\n",
    "            f[k] = {}\n",
    "            for i in range(self.num_nodes):\n",
    "                f[k][i] = {}\n",
    "                for j in self.G.neighbors(i):\n",
    "                    f[k][i][j] = cp.Variable(nonneg=True)\n",
    "        \n",
    "        # Create variable for the maximum concurrent flow\n",
    "        # This represents how much flow each session can carry\n",
    "        mcf = cp.Variable(nonneg=True)\n",
    "        \n",
    "        # Objective: Maximize the concurrent flow\n",
    "        objective = cp.Maximize(mcf)\n",
    "        \n",
    "        # Constraints\n",
    "        constraints = []\n",
    "        \n",
    "        # 1. Capacity constraints for each edge (sum of all commodity flows <= edge capacity)\n",
    "        for u, v in self.edges:\n",
    "            edge_flow_sum = 0\n",
    "            for k in range(self.num_commodities):\n",
    "                edge_flow_sum += f[k][u][v] + f[k][v][u]  # Sum both directions for undirected\n",
    "            constraints.append(edge_flow_sum <= self.G[u][v].get('capacity', float('inf')))\n",
    "        \n",
    "        # 2. Flow conservation constraints\n",
    "        for k, (source, target) in enumerate(self.sessions):\n",
    "            for i in range(self.num_nodes):\n",
    "                if i != source and i != target:  # For intermediate nodes\n",
    "                    # Sum of incoming flows equals sum of outgoing flows\n",
    "                    flow_balance = 0\n",
    "                    for j in self.G.neighbors(i):\n",
    "                        flow_balance += f[k][j][i] - f[k][i][j]\n",
    "                    constraints.append(flow_balance == 0)\n",
    "        \n",
    "        # 3. Flow requirements - each session must achieve the mcf value\n",
    "        for k, (source, target) in enumerate(self.sessions):\n",
    "            # Calculate the net outflow at source\n",
    "            source_outflow = 0\n",
    "            for j in self.G.neighbors(source):\n",
    "                source_outflow += f[k][source][j] - f[k][j][source]\n",
    "            constraints.append(source_outflow == mcf)\n",
    "            \n",
    "            # The net inflow at target should equal the outflow at source\n",
    "            target_inflow = 0\n",
    "            for i in self.G.neighbors(target):\n",
    "                target_inflow += f[k][i][target] - f[k][target][i]\n",
    "            constraints.append(target_inflow == mcf)\n",
    "        \n",
    "        # Solve the problem\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        problem.solve(solver=cp.ECOS)\n",
    "        \n",
    "        # Store results\n",
    "        self.status = problem.status\n",
    "        self.max_concurrent_flow = mcf.value\n",
    "        \n",
    "        # Extract flow values for each commodity on each edge\n",
    "        self.flows = {}\n",
    "        for k in range(self.num_commodities):\n",
    "            self.flows[k] = {}\n",
    "            for u in range(self.num_nodes):\n",
    "                for v in self.G.neighbors(u):\n",
    "                    if (u, v) not in self.flows[k]:\n",
    "                        # Get the flow values in both directions\n",
    "                        forward_flow = f[k][u][v].value\n",
    "                        backward_flow = f[k][v][u].value if v in f[k] and u in f[k][v] else 0\n",
    "                        \n",
    "                        # Calculate effective flow (can't have flow in both directions simultaneously)\n",
    "                        if forward_flow > backward_flow:\n",
    "                            self.flows[k][(u, v)] = forward_flow - backward_flow\n",
    "                            self.flows[k][(v, u)] = 0\n",
    "                        else:\n",
    "                            self.flows[k][(v, u)] = backward_flow - forward_flow\n",
    "                            self.flows[k][(u, v)] = 0\n",
    "        \n",
    "        return {\n",
    "            'status': self.status,\n",
    "            'max_concurrent_flow': self.max_concurrent_flow,\n",
    "            'flows': self.flows\n",
    "        }\n",
    "    \n",
    "    def get_flow_dict(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Returns a dictionary of flows for each session and edge.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: {session_idx: {(u, v): flow_value}}\n",
    "        \"\"\"\n",
    "        if self.flows is None:\n",
    "            raise ValueError(\"Problem must be solved before getting flows\")\n",
    "        return self.flows\n",
    "    \n",
    "    def visualize_network(self, figsize=(12, 8)):\n",
    "        \"\"\"\n",
    "        Visualize the network with edge capacities and optimal flows.\n",
    "        \"\"\"\n",
    "        if self.flows is None:\n",
    "            raise ValueError(\"Problem must be solved before visualization\")\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Create position layout\n",
    "        pos = nx.spring_layout(self.G, seed=42)\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(self.G, pos, node_size=500)\n",
    "        \n",
    "        # Draw edges with capacity labels\n",
    "        edge_labels = {(u, v): f\"cap: {self.G[u][v].get('capacity', '∞')}\" for u, v in self.edges}\n",
    "        nx.draw_networkx_edges(self.G, pos, width=1.0, alpha=0.5)\n",
    "        nx.draw_networkx_edge_labels(self.G, pos, edge_labels=edge_labels)\n",
    "        \n",
    "        # Draw node labels\n",
    "        nx.draw_networkx_labels(self.G, pos)\n",
    "        \n",
    "        # Add source and target information\n",
    "        for k, (source, target) in enumerate(self.sessions):\n",
    "            plt.annotate(f\"Session {k}: {source}→{target} (flow: {self.max_concurrent_flow:.4f})\",\n",
    "                        xy=(0, 0), xytext=(0, -30 - 10 * k),\n",
    "                        xycoords=('axes fraction'), textcoords='offset points',\n",
    "                        ha='left', va='top')\n",
    "        \n",
    "        plt.title(f\"Maximum Concurrent Flow: {self.max_concurrent_flow:.4f}\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def print_solution(self):\n",
    "        \"\"\"\n",
    "        Print the solution details.\n",
    "        \"\"\"\n",
    "        if self.flows is None:\n",
    "            raise ValueError(\"Problem must be solved before printing solution\")\n",
    "        \n",
    "        print(f\"Solution status: {self.status}\")\n",
    "        print(f\"Maximum concurrent flow: {self.max_concurrent_flow:.4f}\")\n",
    "        \n",
    "        for k, (source, target) in enumerate(self.sessions):\n",
    "            print(f\"\\nSession {k}: {source} → {target} (flow: {self.max_concurrent_flow:.4f})\")\n",
    "            \n",
    "            # Create a directed flow network for this commodity\n",
    "            flow_net = nx.DiGraph()\n",
    "            for u, v in self.G.edges():\n",
    "                flow_uv = self.flows[k].get((u, v), 0)\n",
    "                flow_vu = self.flows[k].get((v, u), 0)\n",
    "                \n",
    "                if flow_uv > 1e-6:\n",
    "                    flow_net.add_edge(u, v, flow=flow_uv)\n",
    "                if flow_vu > 1e-6:\n",
    "                    flow_net.add_edge(v, u, flow=flow_vu)\n",
    "            \n",
    "            # Find all simple paths from source to target in the flow network\n",
    "            try:\n",
    "                all_paths = list(nx.all_simple_paths(flow_net, source, target))\n",
    "                \n",
    "                # Calculate flow for each path\n",
    "                path_flows = []\n",
    "                remaining_flow = self.max_concurrent_flow\n",
    "                \n",
    "                for path in all_paths:\n",
    "                    if remaining_flow < 1e-6:\n",
    "                        break\n",
    "                        \n",
    "                    # Find the minimum flow on the path\n",
    "                    min_flow = float('inf')\n",
    "                    for i in range(len(path) - 1):\n",
    "                        u, v = path[i], path[i + 1]\n",
    "                        edge_flow = flow_net[u][v].get('flow', 0)\n",
    "                        min_flow = min(min_flow, edge_flow)\n",
    "                    \n",
    "                    # Skip paths with no flow\n",
    "                    if min_flow < 1e-6:\n",
    "                        continue\n",
    "                    \n",
    "                    # Adjust for remaining flow\n",
    "                    path_flow = min(min_flow, remaining_flow)\n",
    "                    path_flows.append((path, path_flow))\n",
    "                    remaining_flow -= path_flow\n",
    "                    \n",
    "                    # Reduce the flow on this path\n",
    "                    for i in range(len(path) - 1):\n",
    "                        u, v = path[i], path[i + 1]\n",
    "                        flow_net[u][v]['flow'] -= path_flow\n",
    "                \n",
    "                # Print paths\n",
    "                total_session_flow = 0\n",
    "                for path, flow in path_flows:\n",
    "                    print(f\"  Path: {' → '.join(map(str, path))}, Flow: {flow:.4f}\")\n",
    "                    total_session_flow += flow\n",
    "                \n",
    "                print(f\"  Total session flow: {total_session_flow:.4f}\")\n",
    "                \n",
    "            except nx.NetworkXNoPath:\n",
    "                print(f\"  No flow path found from {source} to {target}\")\n",
    "                print(f\"  Total session flow: 0.0000\")\n",
    "    \n",
    "    def visualize_flows(self, figsize=(15, 10)):\n",
    "        \"\"\"\n",
    "        Visualize the network with flows for each session.\n",
    "        \"\"\"\n",
    "        if self.flows is None:\n",
    "            raise ValueError(\"Problem must be solved before visualization\")\n",
    "        \n",
    "        # Create a separate visualization for each session\n",
    "        for k, (source, target) in enumerate(self.sessions):\n",
    "            plt.figure(figsize=figsize)\n",
    "            pos = nx.spring_layout(self.G, seed=42)\n",
    "            \n",
    "            # Create a directed graph for this commodity's flow\n",
    "            flow_graph = nx.DiGraph()\n",
    "            flow_graph.add_nodes_from(self.G.nodes())\n",
    "            \n",
    "            # Add edges with flow values\n",
    "            edge_colors = []\n",
    "            edge_widths = []\n",
    "            \n",
    "            for u, v in self.G.edges():\n",
    "                flow_uv = self.flows[k].get((u, v), 0)\n",
    "                flow_vu = self.flows[k].get((v, u), 0)\n",
    "                \n",
    "                if flow_uv > 1e-6:\n",
    "                    flow_graph.add_edge(u, v, weight=flow_uv)\n",
    "                    edge_colors.append(flow_uv)\n",
    "                    edge_widths.append(1 + 5 * flow_uv / self.max_concurrent_flow if self.max_concurrent_flow > 0 else 1)\n",
    "                \n",
    "                if flow_vu > 1e-6:\n",
    "                    flow_graph.add_edge(v, u, weight=flow_vu)\n",
    "                    edge_colors.append(flow_vu)\n",
    "                    edge_widths.append(1 + 5 * flow_vu / self.max_concurrent_flow if self.max_concurrent_flow > 0 else 1)\n",
    "            \n",
    "            # Draw the nodes\n",
    "            nx.draw_networkx_nodes(flow_graph, pos, node_size=700,\n",
    "                                  node_color=['red' if n == source else 'green' if n == target else 'lightblue' \n",
    "                                             for n in flow_graph.nodes()])\n",
    "            \n",
    "            # Draw the edges with flow\n",
    "            if flow_graph.edges():\n",
    "                edges = nx.draw_networkx_edges(flow_graph, pos, width=edge_widths, edge_color=edge_colors, \n",
    "                                             edge_cmap=plt.cm.Blues, edge_vmin=0, edge_vmax=self.max_concurrent_flow,\n",
    "                                             connectionstyle='arc3,rad=0.1')  # Curved edges for directed\n",
    "                \n",
    "                # Add a colorbar\n",
    "                sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(0, self.max_concurrent_flow))\n",
    "                sm.set_array([])\n",
    "                cbar = plt.colorbar(sm)\n",
    "                cbar.set_label('Flow Amount')\n",
    "                \n",
    "                # Add edge labels with flow values\n",
    "                edge_labels = {(u, v): f\"{flow_graph[u][v]['weight']:.2f}\" for u, v in flow_graph.edges()}\n",
    "                nx.draw_networkx_edge_labels(flow_graph, pos, edge_labels=edge_labels, label_pos=0.3)\n",
    "            \n",
    "            # Draw node labels\n",
    "            nx.draw_networkx_labels(flow_graph, pos)\n",
    "            \n",
    "            # Set title\n",
    "            plt.title(f\"Session {k}: {source} → {target} (Flow: {self.max_concurrent_flow:.4f})\")\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def run_example():\n",
    "    # Create a sample network\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes and edges with capacities\n",
    "    edges = [\n",
    "        (0,3),\n",
    "        (1,3),\n",
    "        (2,3),\n",
    "        (0,4),\n",
    "        (1,4),\n",
    "        (2,4)\n",
    "    ]\n",
    "    for u, v in edges:\n",
    "        G.add_edge(u, v, capacity=1)\n",
    "    \n",
    "    # Define sessions: (source, target)\n",
    "    sessions = [\n",
    "        (0, 1),\n",
    "        (1, 2),\n",
    "        (0, 2),\n",
    "        (3, 4)\n",
    "    ]\n",
    "    \n",
    "    # Create and solve the maximum concurrent flow problem\n",
    "    solver = MaxConcurrentFlowSolver(G, sessions)\n",
    "    result = solver.solve()\n",
    "    \n",
    "    # Display results\n",
    "    solver.print_solution()\n",
    "    # solver.visualize_network()\n",
    "    # solver.visualize_flows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 H_d_b + H_e_b\n",
      "X2 + X3 H_d_c + H_e_c\n",
      "X4 H_a_e + H_b_e + H_c_e\n",
      "\n",
      "====== Network Capacity Analysis Results ======\n",
      "Network: 5 nodes, 6 edges\n",
      "Source-sink pairs: [('a', 'b'), ('b', 'c'), ('a', 'c'), ('d', 'e')]\n",
      "\n",
      "Maximum achievable rate: 1.0\n",
      "\n",
      "Key Variable Values:\n",
      "H(X1) = 1.0\n",
      "H(X2) = 1.0\n",
      "H(X3) = 1.0\n",
      "H(X4) = 1.0\n",
      "H_a_d = 0.0\n",
      "H_a_e = 1.0\n",
      "H_d_a = 0.0\n",
      "H_d_b = 1.0\n",
      "H_d_c = 1.0\n",
      "H_e_a = 0.0\n",
      "H_e_b = 0.0\n",
      "H_e_c = 1.0\n",
      "H_b_d = 0.0\n",
      "H_b_e = 0.0\n",
      "H_c_d = 0.0\n",
      "H_c_e = 0.0\n",
      "H_in_a = 1.0\n",
      "H_out_a = 1.0\n",
      "H_in_out_a = 1.0\n",
      "H_in_d = 1.0\n",
      "H_out_d = 1.0\n",
      "H_in_out_d = 1.0\n",
      "H_in_e = 1.0\n",
      "H_out_e = 1.0\n",
      "H_in_out_e = 1.0\n",
      "H_in_b = 1.0\n",
      "H_out_b = 1.0\n",
      "H_in_out_b = 1.0\n",
      "H_in_c = 1.0\n",
      "H_out_c = 1.0\n",
      "H_in_out_c = 1.0\n",
      "H_in_a_d = 1.0\n",
      "H_out_a_d = 1.0\n",
      "H_in_out_a_d = 1.0\n",
      "H_in_a_e = 1.0\n",
      "H_out_a_e = 1.0\n",
      "H_in_out_a_e = 1.0\n",
      "H_in_a_b = 1.0\n",
      "H_out_a_b = 1.0\n",
      "H_in_out_a_b = 1.0\n",
      "H_in_a_c = 1.0\n",
      "H_out_a_c = 1.0\n",
      "H_in_out_a_c = 1.0\n",
      "H_in_d_e = 1.0\n",
      "H_out_d_e = 1.0\n",
      "H_in_out_d_e = 1.0\n",
      "H_in_b_d = 1.0\n",
      "H_out_b_d = 1.0\n",
      "H_in_out_b_d = 1.0\n",
      "H_in_c_d = 1.0\n",
      "H_out_c_d = 1.0\n",
      "H_in_out_c_d = 1.0\n",
      "H_in_b_e = 1.0\n",
      "H_out_b_e = 1.0\n",
      "H_in_out_b_e = 1.0\n",
      "H_in_c_e = 1.0\n",
      "H_out_c_e = 1.0\n",
      "H_in_out_c_e = 1.0\n",
      "H_in_b_c = 1.0\n",
      "H_out_b_c = 1.0\n",
      "H_in_out_b_c = 1.0\n",
      "H_in_a_d_e = 1.0\n",
      "H_out_a_d_e = 1.0\n",
      "H_in_out_a_d_e = 1.0\n",
      "H_in_a_b_d = 1.0\n",
      "H_out_a_b_d = 1.0\n",
      "H_in_out_a_b_d = 1.0\n",
      "H_in_a_c_d = 1.0\n",
      "H_out_a_c_d = 1.0\n",
      "H_in_out_a_c_d = 1.0\n",
      "H_in_a_b_e = 1.0\n",
      "H_out_a_b_e = 1.0\n",
      "H_in_out_a_b_e = 1.0\n",
      "H_in_a_c_e = 1.0\n",
      "H_out_a_c_e = 1.0\n",
      "H_in_out_a_c_e = 1.0\n",
      "H_in_a_b_c = 1.0\n",
      "H_out_a_b_c = 1.0\n",
      "H_in_out_a_b_c = 1.0\n",
      "H_in_b_d_e = 1.0\n",
      "H_out_b_d_e = 1.0\n",
      "H_in_out_b_d_e = 1.0\n",
      "H_in_c_d_e = 1.0\n",
      "H_out_c_d_e = 1.0\n",
      "H_in_out_c_d_e = 1.0\n",
      "H_in_b_c_d = 1.0\n",
      "H_out_b_c_d = 1.0\n",
      "H_in_out_b_c_d = 1.0\n",
      "H_in_b_c_e = 1.0\n",
      "H_out_b_c_e = 1.0\n",
      "H_in_out_b_c_e = 1.0\n",
      "H_in_a_b_d_e = 1.0\n",
      "H_out_a_b_d_e = 1.0\n",
      "H_in_out_a_b_d_e = 1.0\n",
      "H_in_a_c_d_e = 1.0\n",
      "H_out_a_c_d_e = 1.0\n",
      "H_in_out_a_c_d_e = 1.0\n",
      "H_in_a_b_c_d = 1.0\n",
      "H_out_a_b_c_d = 1.0\n",
      "H_in_out_a_b_c_d = 1.0\n",
      "H_in_a_b_c_e = 1.0\n",
      "H_out_a_b_c_e = 1.0\n",
      "H_in_out_a_b_c_e = 1.0\n",
      "H_in_b_c_d_e = 1.0\n",
      "\n",
      "===========================================\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "from pulp import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EntropyCalculusAnalyzer:\n",
    "    \"\"\"\n",
    "    A general script for analyzing network capacity using entropy calculus.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.G = None  # Undirected graph\n",
    "        self.DG = None  # Directed graph\n",
    "        self.source_sink_pairs = []\n",
    "        self.entropy_vars = {}  # Dictionary to store all entropy variables\n",
    "        self.results = {}\n",
    "        \n",
    "    def create_network(self, edges, capacities=None):\n",
    "        \"\"\"\n",
    "        Create a network from a list of edges.\n",
    "        \n",
    "        Args:\n",
    "            edges: List of (u, v) tuples representing edges\n",
    "            capacities: Dictionary mapping edges to capacities, defaults to 1\n",
    "        \"\"\"\n",
    "        self.G = nx.Graph()\n",
    "        self.G.add_edges_from(edges)\n",
    "        \n",
    "        # Set capacities\n",
    "        if capacities:\n",
    "            for edge, capacity in capacities.items():\n",
    "                u, v = edge\n",
    "                self.G[u][v]['capacity'] = capacity\n",
    "        else:\n",
    "            # Default capacity is 1\n",
    "            nx.set_edge_attributes(self.G, 1, 'capacity')\n",
    "            \n",
    "        # Create directed version\n",
    "        self.DG = self._derive_directed_graph()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_source_sink_pairs(self, pairs):\n",
    "        \"\"\"\n",
    "        Set the source-sink pairs for the network.\n",
    "        \n",
    "        Args:\n",
    "            pairs: List of (source, sink) tuples\n",
    "        \"\"\"\n",
    "        self.source_sink_pairs = pairs\n",
    "        return self\n",
    "    \n",
    "    def _derive_directed_graph(self):\n",
    "        \"\"\"\n",
    "        Derive a directed graph by replacing each undirected edge\n",
    "        with two directed edges.\n",
    "        \"\"\"\n",
    "        DG = nx.DiGraph()\n",
    "        \n",
    "        # Add all nodes\n",
    "        DG.add_nodes_from(self.G.nodes)\n",
    "        \n",
    "        # Replace each undirected edge with two directed edges\n",
    "        for u, v, data in self.G.edges(data=True):\n",
    "            capacity = data.get('capacity', 1)\n",
    "            DG.add_edge(u, v, capacity=capacity)\n",
    "            DG.add_edge(v, u, capacity=capacity)\n",
    "        \n",
    "        return DG\n",
    "    \n",
    "    def get_all_cuts(self):\n",
    "        \"\"\"\n",
    "        Generate all possible cuts of the graph.\n",
    "        A cut is a partition of nodes into two sets.\n",
    "        \"\"\"\n",
    "        nodes = list(self.G.nodes)\n",
    "        cuts = []\n",
    "        \n",
    "        # Generate all possible subsets of nodes\n",
    "        for i in range(1, len(nodes)):\n",
    "            for subset in itertools.combinations(nodes, i):\n",
    "                S = set(subset)\n",
    "                S_complement = set(nodes) - S\n",
    "                cuts.append((S, S_complement))\n",
    "        \n",
    "        return cuts\n",
    "    \n",
    "    def get_incoming_edges(self, S):\n",
    "        \"\"\"Get incoming edges to a set of nodes in the directed graph.\"\"\"\n",
    "        incoming = []\n",
    "        for v in S:\n",
    "            for u in self.DG.predecessors(v):\n",
    "                if u not in S:\n",
    "                    incoming.append((u, v))\n",
    "        return incoming\n",
    "    \n",
    "    def get_outgoing_edges(self, S):\n",
    "        \"\"\"Get outgoing edges from a set of nodes in the directed graph.\"\"\"\n",
    "        outgoing = []\n",
    "        for u in S:\n",
    "            for v in self.DG.successors(u):\n",
    "                if v not in S:\n",
    "                    outgoing.append((u, v))\n",
    "        return outgoing\n",
    "    \n",
    "    def formulate_lp(self):\n",
    "        \"\"\"\n",
    "        Formulate a linear program for network capacity analysis.\n",
    "        \"\"\"\n",
    "        # Create LP problem\n",
    "        prob = LpProblem(\"Network_Capacity\", LpMaximize)\n",
    "        \n",
    "        # 1. Create variables for all sessions (source-sink pairs)\n",
    "        for i, (source, sink) in enumerate(self.source_sink_pairs):\n",
    "            var_name = f\"X{i+1}\"\n",
    "            self.entropy_vars[var_name] = LpVariable(var_name, lowBound=0)\n",
    "        \n",
    "        # 2. Create variables for all directed edges\n",
    "        for u, v in self.DG.edges():\n",
    "            var_name = f\"H_{u}_{v}\"\n",
    "            self.entropy_vars[var_name] = LpVariable(var_name, lowBound=0)\n",
    "        \n",
    "        # 3. Create variables for all subsets used in input-output inequalities\n",
    "        all_cuts = self.get_all_cuts()\n",
    "        for S, S_complement in all_cuts:\n",
    "            s_label = '_'.join(sorted(S))\n",
    "            # Check if any inputs (edges or sources) or outputs (edges or sinks) exist for S\n",
    "            has_inputs = bool(self.get_incoming_edges(S) or any(s in S for s,t in self.source_sink_pairs))\n",
    "            has_outputs = bool(self.get_outgoing_edges(S) or any(t in S for s,t in self.source_sink_pairs))\n",
    "\n",
    "            if has_inputs:\n",
    "                self.entropy_vars[f\"H_in_{s_label}\"] = LpVariable(f\"H_in_{s_label}\", lowBound=0)\n",
    "            if has_outputs:\n",
    "                 self.entropy_vars[f\"H_out_{s_label}\"] = LpVariable(f\"H_out_{s_label}\", lowBound=0)\n",
    "            # Need H_in_out only if both inputs and outputs exist conceptually for IO inequality\n",
    "            if has_inputs and has_outputs:\n",
    "                 self.entropy_vars[f\"H_in_out_{s_label}\"] = LpVariable(f\"H_in_out_{s_label}\", lowBound=0)\n",
    "\n",
    "        \n",
    "        # 4. Set objective: maximize H(X1)\n",
    "        prob += self.entropy_vars[\"X1\"]\n",
    "        \n",
    "        # 5. Add constraint: all sessions have the same entropy\n",
    "        for i in range(2, len(self.source_sink_pairs) + 1):\n",
    "            prob += self.entropy_vars[f\"X{i}\"] == self.entropy_vars[\"X1\"]\n",
    "        \n",
    "        # 6. Add capacity constraints\n",
    "        for u, v in self.G.edges():\n",
    "            prob += (self.entropy_vars[f\"H_{u}_{v}\"] + self.entropy_vars[f\"H_{v}_{u}\"] \n",
    "                    <= self.G[u][v].get('capacity', 1))\n",
    "        \n",
    "        # 7. Add input-output inequalities for all cuts\n",
    "        for S, S_complement in all_cuts:\n",
    "            s_label = '_'.join(sorted(S))\n",
    "            \n",
    "            # Identify components for this cut\n",
    "            in_edges = self.get_incoming_edges(S)\n",
    "            out_edges = self.get_outgoing_edges(S)\n",
    "            \n",
    "            sources_in_S_idx = [i for i, (s, t) in enumerate(self.source_sink_pairs) if s in S]\n",
    "            sinks_in_S_idx = [i for i, (s, t) in enumerate(self.source_sink_pairs) if t in S]\n",
    "\n",
    "            sources_in_S_vars = [self.entropy_vars[f\"X{i+1}\"] for i in sources_in_S_idx]\n",
    "            sinks_in_S_vars = [self.entropy_vars[f\"X{i+1}\"] for i in sinks_in_S_idx] # Used for H_out and H_in_out bounds\n",
    "\n",
    "            in_edge_vars = [self.entropy_vars[f\"H_{u}_{v}\"] for u, v in in_edges]\n",
    "            out_edge_vars = [self.entropy_vars[f\"H_{u}_{v}\"] for u, v in out_edges]\n",
    "\n",
    "            # Define H_in_S and its relations (if it exists)\n",
    "            in_S_name = f\"H_in_{s_label}\"\n",
    "            if in_S_name in self.entropy_vars:\n",
    "                H_in_S = self.entropy_vars[in_S_name]\n",
    "                # Upper bound relaxation: H_in_S <= sum of components\n",
    "                prob += H_in_S <= lpSum(in_edge_vars) + lpSum(sources_in_S_vars), f\"UpperBound_{in_S_name}\"\n",
    "                # Monotonicity: Each component <= H_in_S\n",
    "                for edge_var in in_edge_vars:\n",
    "                    prob += edge_var <= H_in_S, f\"MonoInEdge_{edge_var.name}_in_{s_label}\"\n",
    "                for source_var in sources_in_S_vars:\n",
    "                    prob += source_var <= H_in_S, f\"MonoInSource_{source_var.name}_in_{s_label}\"\n",
    "\n",
    "            # Define H_out_S and its relations (if it exists)\n",
    "            out_S_name = f\"H_out_{s_label}\"\n",
    "            if out_S_name in self.entropy_vars:\n",
    "                H_out_S = self.entropy_vars[out_S_name]\n",
    "                 # Upper bound relaxation: H_out_S <= sum of components (edges out + sinks in S)\n",
    "                prob += H_out_S <= lpSum(out_edge_vars) + lpSum(sinks_in_S_vars), f\"UpperBound_{out_S_name}\"\n",
    "                 # Monotonicity: Each component <= H_out_S\n",
    "                for edge_var in out_edge_vars:\n",
    "                     prob += edge_var <= H_out_S, f\"MonoOutEdge_{edge_var.name}_out_{s_label}\"\n",
    "                for sink_var in sinks_in_S_vars: # X_i where t_i in S\n",
    "                     prob += sink_var <= H_out_S, f\"MonoOutSink_{sink_var.name}_out_{s_label}\"\n",
    "\n",
    "\n",
    "            # Define H_in_out_S and apply IO (if it exists)\n",
    "            both_S_name = f\"H_in_out_{s_label}\"\n",
    "            if both_S_name in self.entropy_vars:\n",
    "                H_in_out_S = self.entropy_vars[both_S_name]\n",
    "                \n",
    "                # Upper bound relaxation: H_in_out_S <= sum of all components (in/out edges + sources/sinks in S)\n",
    "                prob += H_in_out_S <= lpSum(in_edge_vars) + lpSum(out_edge_vars) + \\\n",
    "                                      lpSum(sources_in_S_vars) + lpSum(sinks_in_S_vars), f\"UpperBound_{both_S_name}\"\n",
    "\n",
    "                # Monotonicity H_in <= H_in_out and H_out <= H_in_out\n",
    "                if in_S_name in self.entropy_vars:\n",
    "                    prob += self.entropy_vars[in_S_name] <= H_in_out_S, f\"Mono_In_vs_InOut_{s_label}\"\n",
    "                if out_S_name in self.entropy_vars:\n",
    "                     prob += self.entropy_vars[out_S_name] <= H_in_out_S, f\"Mono_Out_vs_InOut_{s_label}\"\n",
    "\n",
    "                # Apply Input-Output Inequality Relaxation: H(in U out) <= H(in)\n",
    "                # Use the variables representing the upper bounds\n",
    "                if in_S_name in self.entropy_vars: # Check H_in_S exists\n",
    "                    prob += H_in_out_S <= self.entropy_vars[in_S_name], f\"IO_{s_label}\"\n",
    "\n",
    "\n",
    "        \n",
    "        # 8. Add submodularity constraints\n",
    "        # For each pair of variables S1, S2 where S1 is a subset of S2\n",
    "        # we add H(S1) <= H(S2)\n",
    "\n",
    "        # 9. Add joint decodability constraint per sink\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        # Group session variables by sink node\n",
    "        sink_to_sessions = defaultdict(list)\n",
    "        for i, (source, sink) in enumerate(self.source_sink_pairs):\n",
    "            var_name = f\"X{i+1}\"\n",
    "            sink_to_sessions[sink].append(var_name)\n",
    "        \n",
    "        # For each sink with one or more session targeting it\n",
    "        for sink, session_vars in sink_to_sessions.items():\n",
    "            in_edges = list(self.DG.in_edges(sink))\n",
    "            if in_edges:\n",
    "                lhs = lpSum(self.entropy_vars[x] for x in session_vars)\n",
    "                rhs = lpSum(self.entropy_vars[f\"H_{u}_{v}\"] for u, v in in_edges)\n",
    "                print(lhs,rhs)\n",
    "                prob += lhs <= rhs, f\"Joint_Decodability_at_{sink}\"\n",
    "\n",
    "\n",
    "        \n",
    "        # For session variables in relation to edge variables\n",
    "        for i, (source, sink) in enumerate(self.source_sink_pairs):\n",
    "            var_name = f\"X{i+1}\"\n",
    "            \n",
    "            # Check which cuts have this session's source and sink separated\n",
    "            for S, S_complement in self.get_all_cuts():\n",
    "                if ((source in S and sink in S_complement) or \n",
    "                    (source in S_complement and sink in S)):\n",
    "                    \n",
    "                    # Session entropy should be <= entropy of edges crossing the cut\n",
    "                    in_edges = self.get_incoming_edges(S)\n",
    "                    out_edges = self.get_outgoing_edges(S)\n",
    "                    \n",
    "                    if in_edges and out_edges:\n",
    "                        both_S_name = f\"H_in_out_{'_'.join(sorted(S))}\"\n",
    "                        prob += self.entropy_vars[var_name] <= self.entropy_vars[both_S_name]\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def analyze_network(self):\n",
    "        \"\"\"\n",
    "        Analyze the network by formulating and solving the LP.\n",
    "        \"\"\"\n",
    "        # Formulate LP\n",
    "        prob = self.formulate_lp()\n",
    "        \n",
    "        # Solve with CBC solver\n",
    "        prob.solve(PULP_CBC_CMD(msg=False))\n",
    "        \n",
    "        # Store results\n",
    "        self.results['status'] = LpStatus[prob.status]\n",
    "        \n",
    "        if prob.status == LpStatusOptimal:\n",
    "            self.results['max_rate'] = value(prob.objective)\n",
    "            \n",
    "            # Store variable values\n",
    "            var_values = {}\n",
    "            for name, var in self.entropy_vars.items():\n",
    "                var_values[name] = var.value()\n",
    "            self.results['variables'] = var_values\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "\n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"Print the analysis results.\"\"\"\n",
    "        print(\"\\n====== Network Capacity Analysis Results ======\")\n",
    "        print(f\"Network: {len(self.G.nodes)} nodes, {len(self.G.edges())} edges\")\n",
    "        print(f\"Source-sink pairs: {self.source_sink_pairs}\")\n",
    "        \n",
    "        if 'max_rate' in self.results:\n",
    "            print(f\"\\nMaximum achievable rate: {self.results['max_rate']}\")\n",
    "            \n",
    "            if 'variables' in self.results:\n",
    "                print(\"\\nKey Variable Values:\")\n",
    "                for i in range(len(self.source_sink_pairs)):\n",
    "                    print(f\"H(X{i+1}) = {self.results['variables'][f'X{i+1}']}\")\n",
    "                    \n",
    "                # Print a few edge entropy values as examples\n",
    "                count = 0\n",
    "                for edge, value in self.results['variables'].items():\n",
    "                    if edge.startswith(\"H_\") and \"_\" in edge and count < 100:\n",
    "                        print(f\"{edge} = {value}\")\n",
    "                        count += 1\n",
    "        \n",
    "        print(\"\\n===========================================\")\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create analyzer\n",
    "    analyzer = EntropyCalculusAnalyzer()\n",
    "    \n",
    "    # Example: Create K3,2 network\n",
    "    edges = [\n",
    "        ('a', 'd'), ('a', 'e'),\n",
    "        ('b', 'd'), ('b', 'e'),\n",
    "        ('c', 'd'), ('c', 'e')\n",
    "    ]\n",
    "    analyzer.create_network(edges)\n",
    "    \n",
    "    # Set source-sink pairs (cyclic configuration for K3,2)\n",
    "    pairs = [\n",
    "        ('a', 'b'),  # X1\n",
    "        ('b', 'c'),  # X2\n",
    "        ('a', 'c'),  # X3\n",
    "        ('d', 'e')   # X4\n",
    "    ]\n",
    "    analyzer.set_source_sink_pairs(pairs)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Analyze network\n",
    "    analyzer.analyze_network()\n",
    "    analyzer.print_results()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 14 Crypto Inequality constraint sets.\n",
      "Solving LP...\n",
      "Solver status: Optimal\n",
      "\n",
      "====== Network Capacity Analysis Results ======\n",
      "Network: 5 nodes, 6 edges\n",
      "Source-sink pairs: [('a', 'b'), ('b', 'c'), ('a', 'c'), ('d', 'e')]\n",
      "LP Status: Optimal\n",
      "\n",
      "Maximum achievable rate (Upper Bound): 1.00000000\n",
      "\n",
      "Session Variable Values:\n",
      "H(X1) = 1.00000000\n",
      "H(X2) = 1.00000000\n",
      "H(X3) = 1.00000000\n",
      "H(X4) = 1.00000000\n",
      "\n",
      "Example Edge Variable Values:\n",
      "H_a_d = 1.0000\n",
      "H_a_e = 1.0000\n",
      "H_d_a = 0.0000\n",
      "H_d_b = 0.0000\n",
      "H_d_c = 1.0000\n",
      "H_e_a = 0.0000\n",
      "H_e_b = 1.0000\n",
      "H_e_c = 1.0000\n",
      "H_b_d = 1.0000\n",
      "H_b_e = 0.0000\n",
      "H_c_d = 0.0000\n",
      "H_c_e = 0.0000\n",
      "H_in_a = 1.0000\n",
      "H_out_a = 1.0000\n",
      "H_in_d = 1.0000\n",
      "H_out_d = 1.0000\n",
      "H_in_e = 1.0000\n",
      "H_out_e = 1.0000\n",
      "H_in_b = 1.0000\n",
      "H_out_b = 1.0000\n",
      "\n",
      "Example Cut Variable Values:\n",
      "H_in_out_a = 1.0000\n",
      "H_in_out_d = 1.0000\n",
      "H_in_out_a_d = 1.0000\n",
      "H_in_out_e = 1.0000\n",
      "H_in_out_a_e = 1.0000\n",
      "H_in_out_d_e = 1.0000\n",
      "H_in_out_a_d_e = 1.0000\n",
      "H_in_out_b = 1.0000\n",
      "H_in_out_a_b = 1.0000\n",
      "H_in_out_b_d = 1.0000\n",
      "\n",
      "===========================================\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "from pulp import *\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict # Added import\n",
    "\n",
    "class EntropyCalculusAnalyzer:\n",
    "    \"\"\"\n",
    "    A general script for analyzing network capacity using entropy calculus.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.G = None  # Undirected graph\n",
    "        self.DG = None  # Directed graph\n",
    "        self.source_sink_pairs = []\n",
    "        self.entropy_vars = {}  # Dictionary to store all entropy variables\n",
    "        self.results = {}\n",
    "\n",
    "    def create_network(self, edges, capacities=None):\n",
    "        self.G = nx.Graph()\n",
    "        self.G.add_edges_from(edges)\n",
    "        if capacities:\n",
    "            for edge, capacity in capacities.items():\n",
    "                u, v = edge\n",
    "                self.G[u][v]['capacity'] = capacity\n",
    "        else:\n",
    "            nx.set_edge_attributes(self.G, 1, 'capacity')\n",
    "        self.DG = self._derive_directed_graph()\n",
    "        return self\n",
    "\n",
    "    def set_source_sink_pairs(self, pairs):\n",
    "        self.source_sink_pairs = pairs\n",
    "         # Store mapping for quick lookup\n",
    "        self.source_map = {s: i for i, (s, t) in enumerate(pairs)}\n",
    "        self.sink_map = {t: i for i, (s, t) in enumerate(pairs)}\n",
    "        return self\n",
    "\n",
    "    def _derive_directed_graph(self):\n",
    "        DG = nx.DiGraph()\n",
    "        DG.add_nodes_from(self.G.nodes)\n",
    "        for u, v, data in self.G.edges(data=True):\n",
    "            capacity = data.get('capacity', 1)\n",
    "            DG.add_edge(u, v, capacity=capacity)\n",
    "            DG.add_edge(v, u, capacity=capacity)\n",
    "        return DG\n",
    "\n",
    "    def get_all_cuts(self):\n",
    "        nodes = list(self.G.nodes)\n",
    "        cuts = []\n",
    "        # Limit combinations for very large graphs if needed\n",
    "        max_nodes_for_all_cuts = 10 \n",
    "        num_nodes = len(nodes)\n",
    "        if num_nodes > max_nodes_for_all_cuts:\n",
    "             print(f\"Warning: Graph has {num_nodes} nodes. Calculating all cuts may be slow.\")\n",
    "             # Implement sampling or heuristics if needed for large graphs\n",
    "        \n",
    "        # Generate all non-empty proper subsets\n",
    "        for i in range(1, 1 << num_nodes):\n",
    "             S = set()\n",
    "             for j in range(num_nodes):\n",
    "                  if (i >> j) & 1:\n",
    "                       S.add(nodes[j])\n",
    "             if S and len(S) < num_nodes: # Ensure non-empty proper subset\n",
    "                S_complement = set(nodes) - S\n",
    "                cuts.append((S, S_complement))\n",
    "        # Remove duplicate partitions (e.g., ({a}, {b,c}) and ({b,c}, {a}))\n",
    "        unique_cuts = set()\n",
    "        final_cuts = []\n",
    "        for s_set, sc_set in cuts:\n",
    "            s_tuple = tuple(sorted(list(s_set)))\n",
    "            sc_tuple = tuple(sorted(list(sc_set)))\n",
    "            # Canonical representation (lexicographically smaller first)\n",
    "            cut_repr = tuple(sorted((s_tuple, sc_tuple)))\n",
    "            if cut_repr not in unique_cuts:\n",
    "                unique_cuts.add(cut_repr)\n",
    "                final_cuts.append((s_set, sc_set))\n",
    "\n",
    "        return final_cuts\n",
    "\n",
    "\n",
    "    def get_incoming_edges(self, S):\n",
    "        \"\"\"Get incoming graph edges to a set of nodes S.\"\"\"\n",
    "        incoming = []\n",
    "        for v in S:\n",
    "            for u in self.DG.predecessors(v):\n",
    "                if u not in S:\n",
    "                    incoming.append((u, v))\n",
    "        return incoming\n",
    "\n",
    "    def get_outgoing_edges(self, S):\n",
    "        \"\"\"Get outgoing graph edges from a set of nodes S.\"\"\"\n",
    "        outgoing = []\n",
    "        for u in S:\n",
    "            for v in self.DG.successors(u):\n",
    "                if v not in S:\n",
    "                    outgoing.append((u, v))\n",
    "        return outgoing\n",
    "\n",
    "    def formulate_lp(self):\n",
    "        \"\"\"\n",
    "        Formulate a linear program for network capacity analysis.\n",
    "        \"\"\"\n",
    "        prob = LpProblem(\"Network_Capacity\", LpMaximize)\n",
    "\n",
    "        # 1. Create variables for all sessions (source-sink pairs)\n",
    "        for i, (source, sink) in enumerate(self.source_sink_pairs):\n",
    "            var_name = f\"X{i+1}\"\n",
    "            self.entropy_vars[var_name] = LpVariable(var_name, lowBound=0)\n",
    "\n",
    "        # 2. Create variables for all directed edges\n",
    "        for u, v in self.DG.edges():\n",
    "            var_name = f\"H_{u}_{v}\"\n",
    "            self.entropy_vars[var_name] = LpVariable(var_name, lowBound=0)\n",
    "\n",
    "        # 3. Create variables for auxiliary cut entropies\n",
    "        all_cuts = self.get_all_cuts()\n",
    "        for S, S_complement in all_cuts:\n",
    "            s_label = '_'.join(sorted(S))\n",
    "            # Check if any inputs (edges or sources) or outputs (edges or sinks) exist for S\n",
    "            has_inputs = bool(self.get_incoming_edges(S) or any(s in S for s,t in self.source_sink_pairs))\n",
    "            has_outputs = bool(self.get_outgoing_edges(S) or any(t in S for s,t in self.source_sink_pairs))\n",
    "\n",
    "            if has_inputs:\n",
    "                self.entropy_vars[f\"H_in_{s_label}\"] = LpVariable(f\"H_in_{s_label}\", lowBound=0)\n",
    "            if has_outputs:\n",
    "                 self.entropy_vars[f\"H_out_{s_label}\"] = LpVariable(f\"H_out_{s_label}\", lowBound=0)\n",
    "            # Need H_in_out only if both inputs and outputs exist conceptually for IO inequality\n",
    "            if has_inputs and has_outputs:\n",
    "                 self.entropy_vars[f\"H_in_out_{s_label}\"] = LpVariable(f\"H_in_out_{s_label}\", lowBound=0)\n",
    "\n",
    "\n",
    "        # 4. Set objective: maximize H(X1) (or average rate)\n",
    "        prob += self.entropy_vars[\"X1\"]\n",
    "\n",
    "        # 5. Add constraint: all sessions have the same entropy rate R\n",
    "        session_rate = self.entropy_vars[\"X1\"] # R\n",
    "        for i in range(1, len(self.source_sink_pairs)):\n",
    "            prob += self.entropy_vars[f\"X{i+1}\"] == session_rate\n",
    "\n",
    "        # 6. Add capacity constraints for undirected edges\n",
    "        for u, v in self.G.edges():\n",
    "            # Ensure variables exist before adding constraint\n",
    "            h_uv_name = f\"H_{u}_{v}\"\n",
    "            h_vu_name = f\"H_{v}_{u}\"\n",
    "            if h_uv_name in self.entropy_vars and h_vu_name in self.entropy_vars:\n",
    "                 prob += (self.entropy_vars[h_uv_name] + self.entropy_vars[h_vu_name]\n",
    "                         <= self.G[u][v].get('capacity', 1)), f\"Capacity_{u}_{v}\"\n",
    "\n",
    "        # 7. Add Crypto Inequalities for all cuts\n",
    "        # H(Sessions_crossing_cut | Edges_crossing_cut) = 0\n",
    "        # Relaxed to: sum H(Xi crossing) <= H_cut <= sum H(Edges crossing)\n",
    "        # print(\"Adding Crypto Constraints...\") # Added print statement\n",
    "        crypto_constraints_added = 0\n",
    "        for S, S_complement in all_cuts: # Assuming all_cuts is available\n",
    "            s_label = '_'.join(sorted(S))\n",
    "\n",
    "            # DEM(S): Sessions crossing the cut\n",
    "            dem_S_vars = []\n",
    "            dem_indices = [] # Store indices for variable names if needed\n",
    "            for i, (s, t) in enumerate(self.source_sink_pairs):\n",
    "                if (s in S and t in S_complement) or (s in S_complement and t in S):\n",
    "                    var_name = f\"X{i+1}\"\n",
    "                    if var_name in self.entropy_vars:\n",
    "                        dem_S_vars.append(self.entropy_vars[var_name])\n",
    "                        dem_indices.append(i+1)\n",
    "\n",
    "            # CUT(S): Edges crossing the cut (in both directions)\n",
    "            cut_S_edge_vars = []\n",
    "            cut_edge_names = [] # Store names for debugging if needed\n",
    "            crossing_edges = []\n",
    "            # Edges S -> S_complement\n",
    "            for u in S:\n",
    "                for v in self.DG.successors(u):\n",
    "                    if v in S_complement:\n",
    "                        crossing_edges.append((u,v))\n",
    "            # Edges S_complement -> S\n",
    "            for u in S_complement:\n",
    "                for v in self.DG.successors(u):\n",
    "                    if v in S:\n",
    "                        crossing_edges.append((u,v))\n",
    "\n",
    "            for u, v in crossing_edges:\n",
    "                edge_name = f\"H_{u}_{v}\"\n",
    "                if edge_name in self.entropy_vars:\n",
    "                    cut_S_edge_vars.append(self.entropy_vars[edge_name])\n",
    "                    cut_edge_names.append(edge_name)\n",
    "\n",
    "            # Only add if there are sessions demanding transfer across the cut\n",
    "            # and edges allowing transfer across the cut\n",
    "            if dem_S_vars and cut_S_edge_vars:\n",
    "                # Create auxiliary variable H_cut_S\n",
    "                cut_var_name = f\"H_cut_{s_label}\"\n",
    "                # Avoid recreating if it already exists (can happen with symmetric cuts)\n",
    "                if cut_var_name not in self.entropy_vars:\n",
    "                    H_cut_S = LpVariable(cut_var_name, lowBound=0)\n",
    "                    self.entropy_vars[cut_var_name] = H_cut_S\n",
    "                else:\n",
    "                    H_cut_S = self.entropy_vars[cut_var_name]\n",
    "                \n",
    "                crypto_constraints_added += 1\n",
    "\n",
    "                # Upper bound H_cut_S by sum of crossing edge entropies\n",
    "                prob += H_cut_S <= lpSum(cut_S_edge_vars), f\"CryptoUpperBound_{s_label}\"\n",
    "\n",
    "                # Monotonicity: Each crossing edge entropy <= H_cut_S\n",
    "                for edge_var in cut_S_edge_vars:\n",
    "                    prob += edge_var <= H_cut_S, f\"CryptoMonoEdge_{edge_var.name}_vs_{s_label}\"\n",
    "\n",
    "                # Crypto Inequality Relaxation: sum H(Sessions crossing) <= H_cut_S\n",
    "                prob += lpSum(dem_S_vars) <= H_cut_S, f\"Crypto_{s_label}\"\n",
    "\n",
    "        # Optional: Print how many were added\n",
    "        print(f\"Added {crypto_constraints_added} Crypto Inequality constraint sets.\")\n",
    "\n",
    "\n",
    "\n",
    "        # 7. Add input-output and related monotonicity constraints for all cuts\n",
    "        for S, S_complement in all_cuts:\n",
    "            s_label = '_'.join(sorted(S))\n",
    "            \n",
    "            # Identify components for this cut\n",
    "            in_edges = self.get_incoming_edges(S)\n",
    "            out_edges = self.get_outgoing_edges(S)\n",
    "            \n",
    "            sources_in_S_idx = [i for i, (s, t) in enumerate(self.source_sink_pairs) if s in S]\n",
    "            sinks_in_S_idx = [i for i, (s, t) in enumerate(self.source_sink_pairs) if t in S]\n",
    "\n",
    "            sources_in_S_vars = [self.entropy_vars[f\"X{i+1}\"] for i in sources_in_S_idx]\n",
    "            sinks_in_S_vars = [self.entropy_vars[f\"X{i+1}\"] for i in sinks_in_S_idx] # Used for H_out and H_in_out bounds\n",
    "\n",
    "            in_edge_vars = [self.entropy_vars[f\"H_{u}_{v}\"] for u, v in in_edges]\n",
    "            out_edge_vars = [self.entropy_vars[f\"H_{u}_{v}\"] for u, v in out_edges]\n",
    "\n",
    "            # Define H_in_S and its relations (if it exists)\n",
    "            in_S_name = f\"H_in_{s_label}\"\n",
    "            if in_S_name in self.entropy_vars:\n",
    "                H_in_S = self.entropy_vars[in_S_name]\n",
    "                # Upper bound relaxation: H_in_S <= sum of components\n",
    "                prob += H_in_S <= lpSum(in_edge_vars) + lpSum(sources_in_S_vars), f\"UpperBound_{in_S_name}\"\n",
    "                # Monotonicity: Each component <= H_in_S\n",
    "                for edge_var in in_edge_vars:\n",
    "                    prob += edge_var <= H_in_S, f\"MonoInEdge_{edge_var.name}_in_{s_label}\"\n",
    "                for source_var in sources_in_S_vars:\n",
    "                    prob += source_var <= H_in_S, f\"MonoInSource_{source_var.name}_in_{s_label}\"\n",
    "\n",
    "            # Define H_out_S and its relations (if it exists)\n",
    "            out_S_name = f\"H_out_{s_label}\"\n",
    "            if out_S_name in self.entropy_vars:\n",
    "                H_out_S = self.entropy_vars[out_S_name]\n",
    "                 # Upper bound relaxation: H_out_S <= sum of components (edges out + sinks in S)\n",
    "                prob += H_out_S <= lpSum(out_edge_vars) + lpSum(sinks_in_S_vars), f\"UpperBound_{out_S_name}\"\n",
    "                 # Monotonicity: Each component <= H_out_S\n",
    "                for edge_var in out_edge_vars:\n",
    "                     prob += edge_var <= H_out_S, f\"MonoOutEdge_{edge_var.name}_out_{s_label}\"\n",
    "                for sink_var in sinks_in_S_vars: # X_i where t_i in S\n",
    "                     prob += sink_var <= H_out_S, f\"MonoOutSink_{sink_var.name}_out_{s_label}\"\n",
    "\n",
    "\n",
    "            # Define H_in_out_S and apply IO (if it exists)\n",
    "            both_S_name = f\"H_in_out_{s_label}\"\n",
    "            if both_S_name in self.entropy_vars:\n",
    "                H_in_out_S = self.entropy_vars[both_S_name]\n",
    "                \n",
    "                # Upper bound relaxation: H_in_out_S <= sum of all components (in/out edges + sources/sinks in S)\n",
    "                prob += H_in_out_S <= lpSum(in_edge_vars) + lpSum(out_edge_vars) + \\\n",
    "                                      lpSum(sources_in_S_vars) + lpSum(sinks_in_S_vars), f\"UpperBound_{both_S_name}\"\n",
    "\n",
    "                # Monotonicity H_in <= H_in_out and H_out <= H_in_out\n",
    "                if in_S_name in self.entropy_vars:\n",
    "                    prob += self.entropy_vars[in_S_name] <= H_in_out_S, f\"Mono_In_vs_InOut_{s_label}\"\n",
    "                if out_S_name in self.entropy_vars:\n",
    "                     prob += self.entropy_vars[out_S_name] <= H_in_out_S, f\"Mono_Out_vs_InOut_{s_label}\"\n",
    "\n",
    "                # Apply Input-Output Inequality Relaxation: H(in U out) <= H(in)\n",
    "                # Use the variables representing the upper bounds\n",
    "                if in_S_name in self.entropy_vars: # Check H_in_S exists\n",
    "                    prob += H_in_out_S <= self.entropy_vars[in_S_name], f\"IO_{s_label}\"\n",
    "\n",
    "\n",
    "        # 8. Add submodularity constraints (Placeholder - complex to add all)\n",
    "        # Example: H(A) + H(B) >= H(A U B) + H(A n B)\n",
    "        # This is partially captured by the monotonicity constraints added above.\n",
    "        # Adding all submodularity constraints is usually infeasible.\n",
    "\n",
    "        # 9. Add joint decodability constraint per sink (Relaxation: sum H(Xi) <= sum H(in_edges))\n",
    "        sink_to_sessions = defaultdict(list)\n",
    "        for i, (source, sink) in enumerate(self.source_sink_pairs):\n",
    "            var_name = f\"X{i+1}\"\n",
    "            sink_to_sessions[sink].append(self.entropy_vars[var_name])\n",
    "\n",
    "        for sink, session_vars_at_sink in sink_to_sessions.items():\n",
    "            in_edges_to_sink = list(self.DG.in_edges(sink))\n",
    "            if in_edges_to_sink: # Only if sink has incoming edges\n",
    "                in_edge_vars_to_sink = [self.entropy_vars[f\"H_{u}_{v}\"] for u, v in in_edges_to_sink]\n",
    "                # Constraint: Sum(H(Xi) for sink) <= Sum(H(Y_uv) for incoming edges)\n",
    "                prob += lpSum(session_vars_at_sink) <= lpSum(in_edge_vars_to_sink), f\"JointDecodability_{sink}\"\n",
    "\n",
    "        # 10. Relate Session variables to Cut entropy (Monotonicity: Xi <= H(Cut))\n",
    "        for i, (source, sink) in enumerate(self.source_sink_pairs):\n",
    "            session_var = self.entropy_vars[f\"X{i+1}\"]\n",
    "\n",
    "            for S, S_complement in all_cuts:\n",
    "                if ((source in S and sink in S_complement) or\n",
    "                    (source in S_complement and sink in S)):\n",
    "                    # If session (i) crosses cut S, then H(Xi) <= H(delta_in(S) U delta_out(S))\n",
    "                    # Use H_in_out_S as the upper bound variable\n",
    "                    s_label = '_'.join(sorted(S))\n",
    "                    both_S_name = f\"H_in_out_{s_label}\"\n",
    "                    if both_S_name in self.entropy_vars: # Check variable exists\n",
    "                         prob += session_var <= self.entropy_vars[both_S_name], f\"SessionCut_{session_var.name}_vs_{s_label}\"\n",
    "\n",
    "        return prob\n",
    "\n",
    "    def analyze_network(self):\n",
    "        \"\"\"\n",
    "        Analyze the network by formulating and solving the LP.\n",
    "        \"\"\"\n",
    "        prob = self.formulate_lp()\n",
    "        \n",
    "        # Optional: Write LP to file for debugging\n",
    "        # prob.writeLP(\"network_capacity.lp\") \n",
    "        \n",
    "        print(\"Solving LP...\")\n",
    "        # Solve with CBC solver, enable messages for debugging if needed\n",
    "        prob.solve(PULP_CBC_CMD(msg=False)) # Set msg=True to see solver output\n",
    "        \n",
    "        print(f\"Solver status: {LpStatus[prob.status]}\")\n",
    "        \n",
    "        self.results['status'] = LpStatus[prob.status]\n",
    "        if prob.status == LpStatusOptimal:\n",
    "            self.results['max_rate'] = value(prob.objective)\n",
    "            var_values = {}\n",
    "            for name, var in self.entropy_vars.items():\n",
    "                var_values[name] = var.value()\n",
    "            self.results['variables'] = var_values\n",
    "        elif prob.status == LpStatusInfeasible:\n",
    "             print(\"Error: The LP problem is infeasible. Check constraints.\")\n",
    "             self.results['max_rate'] = None\n",
    "        else:\n",
    "             print(f\"Warning: Optimal solution not found. Status: {LpStatus[prob.status]}\")\n",
    "             try:\n",
    "                  self.results['max_rate'] = value(prob.objective) # May be None or incorrect\n",
    "             except:\n",
    "                  self.results['max_rate'] = None\n",
    "\n",
    "\n",
    "        return self.results\n",
    "\n",
    "    def print_results(self):\n",
    "        \"\"\"Print the analysis results.\"\"\"\n",
    "        print(\"\\n====== Network Capacity Analysis Results ======\")\n",
    "        print(f\"Network: {len(self.G.nodes)} nodes, {len(self.G.edges())} edges\")\n",
    "        print(f\"Source-sink pairs: {self.source_sink_pairs}\")\n",
    "        print(f\"LP Status: {self.results.get('status', 'N/A')}\")\n",
    "\n",
    "\n",
    "        if 'max_rate' in self.results and self.results['max_rate'] is not None:\n",
    "            print(f\"\\nMaximum achievable rate (Upper Bound): {self.results['max_rate']:.8f}\")\n",
    "\n",
    "            if 'variables' in self.results:\n",
    "                print(\"\\nSession Variable Values:\")\n",
    "                for i in range(len(self.source_sink_pairs)):\n",
    "                     var_name = f\"X{i+1}\"\n",
    "                     val = self.results['variables'].get(var_name, 'N/A')\n",
    "                     print(f\"H({var_name}) = {val if val != 'N/A' else val:.8f}\")\n",
    "\n",
    "                # Optional: Print edge variables or other details if needed for debugging\n",
    "                print(\"\\nExample Edge Variable Values:\")\n",
    "                count = 0\n",
    "                for name, value in self.results['variables'].items():\n",
    "                    if name.startswith(\"H_\") and \"_\" in name and len(name.split('_')) == 3 and count < 20:\n",
    "                        print(f\"{name} = {value:.4f}\")\n",
    "                        count += 1\n",
    "                print(\"\\nExample Cut Variable Values:\")\n",
    "                count = 0\n",
    "                for name, value in self.results['variables'].items():\n",
    "                     if name.startswith(\"H_in_out\") and count < 10:\n",
    "                          print(f\"{name} = {value:.4f}\")\n",
    "                          count += 1\n",
    "\n",
    "        else:\n",
    "             print(\"\\nCould not determine maximum achievable rate.\")\n",
    "\n",
    "        print(\"\\n===========================================\")\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = EntropyCalculusAnalyzer()\n",
    "\n",
    "    edges = [\n",
    "        ('a', 'd'), ('a', 'e'),\n",
    "        ('b', 'd'), ('b', 'e'),\n",
    "        ('c', 'd'), ('c', 'e')\n",
    "    ]\n",
    "    analyzer.create_network(edges)\n",
    "\n",
    "    # Use the ACYCLIC configuration from the paper's Theorem 1\n",
    "    pairs = [\n",
    "        ('a', 'b'),  # X1\n",
    "        ('b', 'c'),  # X2\n",
    "        ('a', 'c'),  # X3\n",
    "        ('d', 'e')   # X4\n",
    "    ]\n",
    "    # # Optional: Use the CYCLIC configuration from Theorem 3\n",
    "    # pairs = [\n",
    "    #     ('a', 'b'),  # X1\n",
    "    #     ('b', 'c'),  # X2\n",
    "    #     ('c', 'a'),  # X3  <- Changed\n",
    "    #     ('d', 'e')   # X4\n",
    "    # ]\n",
    "    analyzer.set_source_sink_pairs(pairs)\n",
    "\n",
    "    analyzer.analyze_network()\n",
    "    analyzer.print_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
